<!DOCTYPE html>
<!--
Copyright (c) 2024 Isao Sonobe
Released under the MIT license
https://opensource.org/license/mit/
-->
<html>
	<head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<title>Chat with your PDF</title>
		<meta name="description" content="Chat with your PDF">
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@gradio/lite@4.14.1/dist/lite.css" />
		<style>
			html, body {
				margin: 0;
				padding: 0;
				height: 100%;
				background: var(--body-background-fill);
			}

			footer {
				display: none !important;
			}

			#chatbot {
				height: auto !important;
				min-height: 500px;
			}

			#chatbot h1 {
				font-size: 2em;
				margin-block-start: 0.67em;
				margin-block-end: 0em;
				margin-inline-start: 0px;
				margin-inline-end: 0px;
				font-weight: bold;			
			}

			#chatbot h2 {
				font-size: 1.5em;
				margin-block-start: 0.83em;
				margin-block-end: 0em;
				margin-inline-start: 0px;
				margin-inline-end: 0px;
				font-weight: bold;
			}

			#chatbot h3 {
				font-size: 1.17em;
				margin-block-start: 1em;
				margin-block-end: 0em;
				margin-inline-start: 0px;
				margin-inline-end: 0px;
				font-weight: bold;
   			}

   			#chatbot h4 {
				margin-block-start: 1.33em;
				margin-block-end: 0em;
				margin-inline-start: 0px;
				margin-inline-end: 0px;
				font-weight: bold;
			}

   			#chatbot h5 {
				margin-block-start: 1.67em;
				margin-block-end: 0em;
				margin-inline-start: 0px;
				margin-inline-end: 0px;
				font-weight: bold;
			}

   			#chatbot h6 {
				margin-block-start: 1.83em;
				margin-block-end: 0em;
				margin-inline-start: 0px;
				margin-inline-end: 0px;
				font-weight: bold;   			
			}

			/* 
			.chatbot {
				white-space: pre-wrap;
			}
			*/

			.gallery-item > .gallery {
				max-width: 380px;
			}

			#context > label > textarea {
				scrollbar-width: thin !important;
			}

			#cost_info {
				border-style: none !important;
			}

			#cost_info > label > input {
				background: var(--panel-background-fill) !important;
			}
		</style>
	</head>
	<body>
		<gradio-lite>
			<gradio-requirements>
				pdfminer.six==20231228
				pyodide-http==0.2.1
				janome==0.5.0
				rank_bm25==0.2.2
			</gradio-requirements>

			<gradio-file name="chat_history.json">
[[null, "ã‚ˆã†ã“ãï¼ã€€PDFã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å‚ç…§ã—ãªãŒã‚‰å¯¾è©±ã§ãã‚‹ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚\nPDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ãƒ†ã‚­ã‚¹ãƒˆãŒæŠ½å‡ºã•ã‚Œã¾ã™ã€‚\nãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ä¸­ã«{context}ã¨æ›¸ãã¨ã€æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãŒãã®éƒ¨åˆ†ã«åŸ‹ã‚è¾¼ã¾ã‚Œã¦å¯¾è©±ãŒè¡Œã‚ã‚Œã¾ã™ã€‚ä»–ã«ã‚‚PDFã®ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢ã—ã¦å‚ç…§ã—ãŸã‚Šã€ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æŒ‡å®šã—ã¦å‚ç…§ã—ãŸã‚Šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä¸€ç•ªä¸‹ã®Examplesã«ã“ã‚Œã‚‰ã®ä¾‹ãŒã‚ã‚Šã¾ã™ã€‚\nãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ›¸ãã¨ãã«Shift+Enterã‚’å…¥åŠ›ã™ã‚‹ã¨æ”¹è¡Œã§ãã¾ã™ã€‚"]]
			</gradio-file>

			<gradio-file name="app.py" entrypoint>
import os

# Gradioã«ã‚ˆã‚‹ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ã‚’ç„¡åŠ¹åŒ–
os.putenv("GRADIO_ANALYTICS_ENABLED", "False")
os.environ["GRADIO_ANALYTICS_ENABLED"] = "False"

# openaiãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•ã¯ https://github.com/pyodide/pyodide/issues/4292 ã‚’å‚è€ƒã«ã—ã¾ã—ãŸã€‚
import micropip
await micropip.install("https://raw.githubusercontent.com/sonoisa/pyodide_wheels/main/multidict/multidict-4.7.6-py3-none-any.whl", keep_going=True)
await micropip.install("https://raw.githubusercontent.com/sonoisa/pyodide_wheels/main/frozenlist/frozenlist-1.4.0-py3-none-any.whl", keep_going=True)
await micropip.install("https://raw.githubusercontent.com/sonoisa/pyodide_wheels/main/aiohttp/aiohttp-4.0.0a2.dev0-py3-none-any.whl", keep_going=True)
await micropip.install("https://raw.githubusercontent.com/sonoisa/pyodide_wheels/main/openai/openai-1.3.7-py3-none-any.whl", keep_going=True)
await micropip.install("https://raw.githubusercontent.com/sonoisa/pyodide_wheels/main/urllib3/urllib3-2.1.0-py3-none-any.whl", keep_going=True)
await micropip.install("ssl")
import ssl
await micropip.install("httpx", keep_going=True)
import httpx
await micropip.install("https://raw.githubusercontent.com/sonoisa/pyodide_wheels/main/urllib3/urllib3-2.1.0-py3-none-any.whl", keep_going=True)
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
await micropip.install("https://raw.githubusercontent.com/sonoisa/pyodide_wheels/main/tiktoken/tiktoken-0.5.1-cp311-cp311-emscripten_3_1_45_wasm32.whl", keep_going=True)


import gradio as gr
import base64
import json
import unicodedata
import re
from pathlib import Path
from dataclasses import dataclass
import asyncio

import pyodide_http
pyodide_http.patch_all()

from pdfminer.pdfinterp import PDFResourceManager
from pdfminer.converter import TextConverter
from pdfminer.pdfinterp import PDFPageInterpreter
from pdfminer.pdfpage import PDFPage
from pdfminer.layout import LAParams
from io import StringIO

from janome.tokenizer import Tokenizer as JanomeTokenizer
from janome.analyzer import Analyzer as JanomeAnalyzer
from janome.tokenfilter import POSStopFilter, LowerCaseFilter
from rank_bm25 import BM25Okapi

from openai import OpenAI, AzureOpenAI
import tiktoken


class URLLib3Transport(httpx.BaseTransport):
	"""
	urllib3ã‚’ä½¿ç”¨ã—ã¦httpxã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ãƒˆã‚¯ãƒ©ã‚¹
	"""
	def __init__(self):
		self.pool = urllib3.PoolManager()

	def handle_request(self, request: httpx.Request):
		payload = json.loads(request.content.decode("utf-8"))
		urllib3_response = self.pool.request(request.method, str(request.url), headers=request.headers, json=payload)
		stream = httpx.ByteStream(urllib3_response.data)
		return httpx.Response(urllib3_response.status, headers=urllib3_response.headers, stream=stream)

http_client = httpx.Client(transport=URLLib3Transport())


@dataclass
class Page:
	"""
	PDFã®ãƒšãƒ¼ã‚¸å†…å®¹
	"""
	number: int
	content: str


OPENAI_TOKENIZER = tiktoken.get_encoding("cl100k_base")
JANOME_TOKENIZER = JanomeTokenizer()
JANOME_ANALYZER = JanomeAnalyzer(tokenizer=JANOME_TOKENIZER, 
	token_filters=[POSStopFilter(["è¨˜å·,ç©ºç™½"]), LowerCaseFilter()])


def extract_pdf_pages(pdf_filename):
	"""
	PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã€‚

	Args:
		pdf_filename (str): æŠ½å‡ºã™ã‚‹PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

	Returns:
		list[Page]: PDFã®å„ãƒšãƒ¼ã‚¸å†…å®¹ã®ãƒªã‚¹ãƒˆ
	"""

	pages = []
	with open(pdf_filename, "rb") as pdf_file:
		output = StringIO()
		resource_manager = PDFResourceManager()
		laparams = LAParams()
		text_converter = TextConverter(resource_manager, output, laparams=laparams)
		page_interpreter = PDFPageInterpreter(resource_manager, text_converter)

		page_number = 0
		for i_page in PDFPage.get_pages(pdf_file):
			try:
				page_number += 1
				page_interpreter.process_page(i_page)
				page_content = output.getvalue()
				page_content = unicodedata.normalize('NFKC', page_content)
				pages.append(Page(number=page_number, content=page_content))
				output.truncate(0)
				output.seek(0)
			except Exception as e:
				print(e)
				pass

		output.close()
		text_converter.close()

		return pages


def merge_pages_with_page_tag(pages):
	"""
	PDFã®å„ãƒšãƒ¼ã‚¸å†…å®¹ã‚’ä¸€ã¤ã®æ–‡å­—åˆ—ã«ãƒãƒ¼ã‚¸ã™ã‚‹ã€‚
	ãŸã ã—ã€chatpdf:pageã¨ã„ã†ã‚¿ã‚°ã§ãƒšãƒ¼ã‚¸ã‚’æ‹¬ã‚‹ã€‚
	extract_pages_from_page_tag()ã®é€†å¤‰æ›ã§ã‚ã‚‹ã€‚

	Args:
		pages (list[Page]): PDFã®å„ãƒšãƒ¼ã‚¸å†…å®¹ã®ãƒªã‚¹ãƒˆ

	Returns:
		str: PDFã®å„ãƒšãƒ¼ã‚¸å†…å®¹ã‚’ãƒãƒ¼ã‚¸ã—ãŸæ–‡å­—åˆ—
	"""
	document_with_page_tag = ""
	for page in pages:
		document_with_page_tag += f'&lt;chatpdf:page number="{page.number}"&gt;\n{page.content}\n&lt;/chatpdf:page&gt;\n'
	return document_with_page_tag


def extract_pages_from_page_tag(document_with_page_tag):
	"""
	chatpdf:pageã¨ã„ã†ã‚¿ã‚°ã§æ‹¬ã‚‰ã‚ŒãŸé ˜åŸŸã‚’PDFã®ãƒšãƒ¼ã‚¸å†…å®¹ã¨è§£é‡ˆã—ã¦ã€Pageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹ã€‚
	merge_pages_with_page_tag()ã®é€†å¤‰æ›ã§ã‚ã‚‹ã€‚

	Args:
		document_with_page_tag (str): chatpdf:pageã¨ã„ã†ã‚¿ã‚°ã§å„ãƒšãƒ¼ã‚¸ãŒæ‹¬ã‚‰ã‚ŒãŸæ–‡å­—åˆ—

	Returns:
		list[Page]: Pageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆ
	"""
	page_tag_pattern = r'&lt;chatpdf:page number="(\d+)"&gt;\n?(.*?)\n?&lt;\/chatpdf:page&gt;\n?'
	matches = re.findall(page_tag_pattern, document_with_page_tag, re.DOTALL)
	pages = [Page(number=int(number), content=content) for number, content in matches]
	return pages


def escape_latex(unescaped_text):
	"""
	Chatbotã®markdownã§æ•°å¼ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã‚ˆã†ã« \\(, \\), \\[, \\] ã‚’ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã§ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã™ã‚‹ã€‚

	Args:
		unescaped_text (str): ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å¯¾è±¡æ–‡å­—åˆ—

	Returns:
		str: ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚ŒãŸæ–‡å­—åˆ—
	"""
	return re.sub(r"(\\[\(\)\[\]])", r"\\\1", unescaped_text)


def unescape_latex(escaped_text):
	"""
	Chatbotã®markdownã§æ•°å¼ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã‚ˆã†ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚Œã¦ã„ãŸ \\(, \\), \\[, \\] ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚Œã¦ã„ãªã„å…ƒã®æ‹¬å¼§ã«å¤‰æ›ã™ã‚‹ã€‚

	Args:
		escaped_text (str): ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚ŒãŸæ–‡å­—åˆ—

	Returns:
		str: ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚Œã¦ã„ãªã„æ–‡å­—åˆ—
	"""
	return re.sub(r"\\(\\[\(\)\[\]])", r"\1", escaped_text)


def add_s(values):
	"""
	è¤‡æ•°å½¢ã®sã‚’å¿…è¦ã«å¿œã˜ã¦ä»˜ã‘ã‚‹ãŸã‚ã«ç”¨ã„ã‚‹é–¢æ•°ã€‚
	ä¸ãˆã‚‰ã‚ŒãŸãƒªã‚¹ãƒˆã®è¦ç´ æ•°ãŒ2ä»¥ä¸Šãªã‚‰"s"ã‚’è¿”ã—ã€ãã‚Œä»¥å¤–ã¯""ã‚’è¿”ã™ã€‚

	Args:
		values (list[any]): ãƒªã‚¹ãƒˆ

	Returns:
		str: è¦ç´ æ•°ãŒè¤‡æ•°ãªã‚‰"s"ã€ãã‚Œä»¥å¤–ã¯""
	"""
	return "s" if len(values) > 1 else ""


def get_context_info(characters, tokens):
	"""
	æ–‡å­—æ•°ã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æƒ…å ±ã‚’æ–‡å­—åˆ—ã§è¿”ã™ã€‚

	Args:
		characters (str): ãƒ†ã‚­ã‚¹ãƒˆ
		tokens (list[str]): ãƒˆãƒ¼ã‚¯ãƒ³

	Returns:
		str: æ–‡å­—æ•°ã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æƒ…å ±ã‚’å«ã‚€æ–‡å­—åˆ—
	"""
	char_count = len(characters)
	token_count = len(tokens)
	return f"{char_count:,} character{add_s(characters)}\n{token_count:,} token{add_s(tokens)}"


def update_context_element(pdf_file_obj):
	"""
	PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ã‚’æ›´æ–°ã™ã‚‹ã€‚

	Args:
		pdf_file_obj (File): ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸPDFãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

	Returns:
		Tuple: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã«æ ¼ç´ã™ã‚‹æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã¨ã€ãã®æ–‡å­—æ•°æƒ…å ±
	"""
	pages = extract_pdf_pages(pdf_file_obj.name)
	document_with_tag = merge_pages_with_page_tag(pages)
	return gr.update(value=document_with_tag, interactive=True), count_characters(document_with_tag)


def count_characters(document_with_tag):
	"""
	ãƒ†ã‚­ã‚¹ãƒˆã®æ–‡å­—æ•°ã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—ã™ã‚‹ã€‚
	ãŸã ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã¯chatpdf:pageã¨ã„ã†ã‚¿ã‚°ã§ãƒšãƒ¼ã‚¸ãŒæ‹¬ã‚‰ã‚Œã¦ã„ã‚‹ã¨ã™ã‚‹ã€‚

	Args:
		document_with_tag (str): æ–‡å­—æ•°ã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ

	Returns:
		str: æ–‡å­—æ•°ã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æƒ…å ±ã‚’å«ã‚€æ–‡å­—åˆ—
	"""
	text = "".join([page.content for page in extract_pages_from_page_tag(document_with_tag)])

	tokens = OPENAI_TOKENIZER.encode(text)
	return get_context_info(text, tokens)


class SearchEngine:
	"""
	æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³
	"""
	def __init__(self, engine, pages):
		self.engine = engine
		self.pages = pages


SEARCH_ENGINE = None

def create_search_engine(context):
	"""
	æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½œã‚‹ã€‚

	Args:
		context (str): æ¤œç´¢å¯¾è±¡ã¨ãªã‚‹ãƒ†ã‚­ã‚¹ãƒˆã€‚ãŸã ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã¯chatpdf:pageã¨ã„ã†ã‚¿ã‚°ã§ãƒšãƒ¼ã‚¸ãŒæ‹¬ã‚‰ã‚Œã¦ã„ã‚‹ã¨ã™ã‚‹ã€‚
	"""
	global SEARCH_ENGINE

	pages = extract_pages_from_page_tag(context)
	tokenized_pages = []
	original_pages = []
	for page in pages:
		page_content = page.content.strip()
		if page_content:
			tokenized_page = [token.base_form for token in JANOME_ANALYZER.analyze(page_content)]
			if tokenized_page:
				tokenized_pages.append(tokenized_page)
				original_pages.append(page)

	if tokenized_pages:
		bm25 = BM25Okapi(tokenized_pages)
		SEARCH_ENGINE = SearchEngine(engine=bm25, pages=original_pages)
	else:
		SEARCH_ENGINE = None


def search_pages(keywords, page_limit):
	"""
	ä¸ãˆã‚‰ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢ã™ã‚‹ã€‚

	Args:
		keywords (str): æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
		page_limit (int): æ¤œç´¢ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°

	Returns:
		list[Page]: ãƒ’ãƒƒãƒˆã—ãŸãƒšãƒ¼ã‚¸
	"""
	global SEARCH_ENGINE
	if SEARCH_ENGINE is None:
		return []

	tokenized_query = [token.base_form for token in JANOME_ANALYZER.analyze(keywords)]
	if not tokenized_query:
		return []

	found_pages = SEARCH_ENGINE.engine.get_top_n(tokenized_query, SEARCH_ENGINE.pages, n=page_limit)
	return found_pages


def load_pages(page_numbers):
	"""
	ä¸ãˆã‚‰ã‚ŒãŸãƒšãƒ¼ã‚¸ç•ªå·ã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã™ã‚‹ã€‚

	Args:
		page_numbers (list[int]): å–å¾—ã™ã‚‹ãƒšãƒ¼ã‚¸ç•ªå·

	Returns:
		list[Page]: å–å¾—ã—ãŸãƒšãƒ¼ã‚¸
	"""
	global SEARCH_ENGINE
	if SEARCH_ENGINE is None:
		return []

	page_numbers = set(page_numbers)
	found_pages = [page for page in SEARCH_ENGINE.pages if page.number in page_numbers]
	return found_pages


# function callingç”¨ãƒ„ãƒ¼ãƒ«
CHAT_TOOLS = [
	# ãƒšãƒ¼ã‚¸æ¤œç´¢
	{
		"type": "function",
		"function": {
			"name": "search_pages",
			"description": "Searches for pages containing the given keywords.",
			"parameters": {
				"type": "object",
				"properties": {
					"keywords": {
						"type": "string",
						"description": 'Search keywords separated by spaces. For example, "Artificial General Intelligence è‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ".'
					},
					"page_limit": {
						"type": "number",
						"description": "Maximum number of search results to return. For example, 3.",
						"minimum": 1
					}
				}
			},
			"required": ["keywords"]
		}
	}, 
	# ãƒšãƒ¼ã‚¸å–å¾—
	{
		"type": "function",
		"function": {
			"name": "load_pages",
			"description": "Loads pages specified by their page numbers.",
			"parameters": {
				"type": "object",
				"properties": {
					"page_numbers": {
						"type": "array",
						"items": {
							"type": "number"
						},
						"description": "List of page numbers to be load",
						"minItems": 1
					}
				}
			},
			"required": ["page_numbers"]
		}
	}
]

# function callingãªã©ã€å›ºå®šã§æ¶ˆè²»ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°
CHAT_TOOLS_TOKENS = 139


def get_openai_messages(prompt, history, context):
	"""
	ä¸ãˆã‚‰ã‚ŒãŸå¯¾è©±ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ã€ChatGPT APIã®å…¥åŠ›ã«ç”¨ã„ã‚‰ã‚Œã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿å½¢å¼ã«å¤‰æ›ã—ã¦è¿”ã™ã€‚

	Args:
		prompt (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
		history (list[list[str]]): ãƒãƒ£ãƒƒãƒˆå±¥æ­´
		context (str): ãƒãƒ£ãƒƒãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

	Returns:
		str: ChatGPT APIã®å…¥åŠ›ã«ç”¨ã„ã‚‰ã‚Œã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿
	"""
	global SEARCH_ENGINE
	if SEARCH_ENGINE is not None:
		context = "".join([page.content for page in SEARCH_ENGINE.pages])

	messages = []
	for user_message, assistant_message in history:
		if user_message is not None and assistant_message is not None:
			user_message = unescape_latex(user_message)
			user_message = user_message.replace("{context}", context)
			assistant_message = unescape_latex(assistant_message)
			messages.append({ "role": "user", "content": user_message })
			messages.append({ "role": "assistant", "content": assistant_message })

	prompt = prompt.replace("{context}", context)
	messages.append({ "role": "user", "content": prompt })

	return messages


# ãã‚Œã¾ã§ã®å…¨å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°
actual_total_cost_prompt = 0

# ãã‚Œã¾ã§ã®å…¨å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°
actual_total_cost_completion = 0


async def process_prompt(prompt, history, context, platform, endpoint, azure_deployment, azure_api_version, api_key, model_name, max_tokens, temperature):
	"""
	ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å‡¦ç†ã—ã€ChatGPTã«ã‚ˆã‚‹ç”Ÿæˆçµæœã‚’è¿”ã™ã€‚

	Args:
		prompt (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
		history (list[list[str]]): ãƒãƒ£ãƒƒãƒˆå±¥æ­´
		context (str): ãƒãƒ£ãƒƒãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
		platform (str): ä½¿ç”¨ã™ã‚‹AIãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
		endpoint (str): AIã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
		azure_deployment (str): Azureã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå
		azure_api_version (str): Azure APIã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³
		api_key (str): APIã‚­ãƒ¼
		model_name (str): ä½¿ç”¨ã™ã‚‹AIãƒ¢ãƒ‡ãƒ«ã®åå‰
		max_tokens (int): ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
		temperature (float): ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã®åº¦åˆã„ã‚’ç¤ºã™æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

	Returns:
		str: ChatGPTã«ã‚ˆã‚‹ç”Ÿæˆçµæœ
	"""
	global actual_total_cost_prompt, actual_total_cost_completion

	try:
		messages = get_openai_messages(prompt, history, context)

		if platform == "OpenAI":
			openai_client = OpenAI(
					base_url=endpoint,
					api_key=api_key,
					http_client=http_client
				)
		else: # Azure
			openai_client = AzureOpenAI(
					azure_endpoint=endpoint,
					api_version=azure_api_version,
					azure_deployment=azure_deployment,
					api_key=api_key,
					http_client=http_client
				)

		completion = openai_client.chat.completions.create(
				messages=messages,
				model=model_name,
				max_tokens=max_tokens,
				temperature=temperature,
				tools=CHAT_TOOLS,
				tool_choice="auto",
				stream=False
			)

		bot_response = ""
		if hasattr(completion, "error"):
			raise gr.Error(completion.error["message"])

		response_message = completion.choices[0].message
		tool_calls = response_message.tool_calls
		actual_total_cost_prompt += completion.usage.prompt_tokens
		actual_total_cost_completion += completion.usage.completion_tokens

		if tool_calls:
			messages.append(response_message)

			for tool_call in tool_calls:
				function_name = tool_call.function.name
				function_args = json.loads(tool_call.function.arguments)
				if function_name == "search_pages":
					# ãƒšãƒ¼ã‚¸æ¤œç´¢
					keywords = function_args.get("keywords").strip()
					page_limit = function_args.get("page_limit") or 3

					bot_response += f'Searching for pages containing the keyword{add_s(keywords.split(" "))} "{keywords}".\n'

					found_pages = search_pages(keywords, page_limit)
					function_response = json.dumps({
							"status": "found" if found_pages else "not found",
							"found_pages": [{
								"page_number": page.number,
								"page_content": page.content
							} for page in found_pages]
						}, ensure_ascii=False)
					messages.append({
							"tool_call_id": tool_call.id,
							"role": "tool",
							"name": function_name,
							"content": function_response
						})
					if found_pages:
						bot_response += f'Found page{add_s(found_pages)}: {", ".join([str(page.number) for page in found_pages])}.\n\n'
					else:
						bot_response += "Page not found.\n\n"

				elif function_name == "load_pages":
					# ãƒšãƒ¼ã‚¸å–å¾—
					page_numbers = function_args.get("page_numbers")

					bot_response += f'Trying to load page{add_s(page_numbers)} {", ".join(map(str, page_numbers))}.\n'

					found_pages = load_pages(page_numbers)
					function_response = json.dumps({
							"status": "found" if found_pages else "not found",
							"found_pages": [{
								"page_number": page.number,
								"page_content": page.content
							} for page in found_pages]
						}, ensure_ascii=False)
					messages.append({
							"tool_call_id": tool_call.id,
							"role": "tool",
							"name": function_name,
							"content": function_response
						})
					if found_pages:
						bot_response += f'Found page{add_s(found_pages)}: {", ".join([str(page.number) for page in found_pages])}.\n\n'
					else:
						bot_response += "Page not found.\n\n"
				else:
					raise gr.Error(f"Unknown function calling '{function_name}'.")

			yield bot_response + "Generating response. Please wait a moment...\n"
			await asyncio.sleep(0.1)

			completion = openai_client.chat.completions.create(
					messages=messages,
					model=model_name,
					max_tokens=max_tokens,
					temperature=temperature,
					stream=False
				)
			actual_total_cost_prompt += completion.usage.prompt_tokens
			actual_total_cost_completion += completion.usage.completion_tokens

			if hasattr(completion, "error"):
				raise gr.Error(completion.error["message"])

			response_message = completion.choices[0].message
			bot_response += response_message.content
			yield bot_response

		else:
			bot_response += response_message.content
			yield bot_response

	except Exception as e:
		if hasattr(e, "message"):
			raise gr.Error(e.message)
		else:
			raise gr.Error(str(e))


def load_api_key(file_obj):
	"""
	APIã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€ã€‚

	Args:
		file_obj (File): APIã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

	Returns:
		str: èª­ã¿è¾¼ã¾ã‚ŒãŸAPIã‚­ãƒ¼æ–‡å­—åˆ—
	"""	
	try:
		with open(file_obj.name, "r", encoding="utf-8") as api_key_file:
			return api_key_file.read().strip()
	except Exception as e:
		raise gr.Error(str(e))


def get_cost_info(prompt_token_count):
	"""
	ãƒãƒ£ãƒƒãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®æ–‡å­—åˆ—ã‚’è¿”ã™ã€‚

	Args:
		prompt_token_count (int): ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå±¥æ­´è¾¼ã¿ï¼‰ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°

	Returns:
		str: ãƒãƒ£ãƒƒãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®æ–‡å­—åˆ—
	"""	
	return f"Estimated input cost: {prompt_token_count + CHAT_TOOLS_TOKENS:,} tokens,  Actual total input cost: {actual_total_cost_prompt:,} tokens,  Actual total output cost: {actual_total_cost_completion:,} tokens"


# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šå€¤
DEFAULT_SETTINGS = {
		"setting_name": "Default", 
		"platform": "OpenAI", 
		"endpoint": "https://api.openai.com/v1", 
		"azure_deployment": "", 
		"azure_api_version": "", 
		"model_name": "gpt-4-turbo-preview", 
		"max_tokens": 4096, 
		"temperature": 0.2, 
		"save_chat_history_to_url": False
	};


def main():
	"""
	ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°ã€‚Gradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’è¨­å®šã—ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•ã™ã‚‹ã€‚
	"""
	try:
		# ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚‚ã‚ã‚‹ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’èª­ã¿å‡ºã™ã€‚
		with open("chat_history.json", "r", encoding="utf-8") as f:
			CHAT_HISTORY = json.load(f)
	except Exception as e:
		print(e)
		CHAT_HISTORY = []

	# localStorageã‹ã‚‰è¨­å®šæƒ…å ±ã‚’ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
	js_define_utilities_and_load_settings = """() =&gt; {
			const KEY_PREFIX = "serverless_chat_with_your_pdf:";

			const loadSettings = () =&gt; {
				const getItem = (key, defaultValue) =&gt; {
					const jsonValue = localStorage.getItem(KEY_PREFIX + key);
					if (jsonValue) {
						return JSON.parse(jsonValue);
					} else {
						return defaultValue;
					}
				};
""" + "".join([f"""
				const default_{setting_key} = {json.dumps(default_value, ensure_ascii=False)};
				const {setting_key} = getItem("{setting_key}", default_{setting_key});
""" for setting_key, default_value in DEFAULT_SETTINGS.items()]) + """
				const serialized_saved_settings = getItem("saved_settings", []);
				const default_saved_settings = [[
""" + ", ".join([f"{json.dumps(default_value, ensure_ascii=False)}" for _, default_value in DEFAULT_SETTINGS.items()]) + """
				]];

				saved_settings = [];
				for (let entry of serialized_saved_settings) {
					saved_settings.push([
							entry["setting_name"] || "", 
							entry["platform"] || default_platform, 
							entry["endpoint"] || default_endpoint, 
							entry["azure_deployment"] || default_azure_deployment, 
							entry["azure_api_version"] || default_azure_api_version, 
							entry["model_name"] || default_model_name, 
							entry["max_tokens"] || default_max_tokens, 
							entry["temperature"] || default_temperature, 
							entry["save_chat_history_to_url"] || default_save_chat_history_to_url
						]);
				}
				if (saved_settings.length == 0) {
					saved_settings = default_saved_settings;
				}

				return [setting_name, platform, endpoint, azure_deployment, azure_api_version, model_name, max_tokens, temperature, save_chat_history_to_url, saved_settings];
			};

			globalThis.resetSettings = () =&gt; {
				for (let key in localStorage) {
					if (key.startsWith(KEY_PREFIX) && !key.startsWith(KEY_PREFIX + "saved_settings") && !key.startsWith(KEY_PREFIX + "setting_name")) {
						localStorage.removeItem(key);
					}
				}

				return loadSettings();
			};

			globalThis.saveItem = (key, value) =&gt; {
				localStorage.setItem(KEY_PREFIX + key, JSON.stringify(value));
			};

			return loadSettings();
		}
		"""

	# should_saveãŒtrueã§ã‚ã‚Œã°URLã«ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜ã—ã€falseã§ã‚ã‚Œã°ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’å‰Šé™¤ã™ã‚‹ã€‚
	save_or_delete_chat_history = '''(hist, should_save) =&gt; {
			saveItem("save_chat_history_to_url", should_save);
			if (!should_save) {
				const url = new URL(window.location.href);
				url.searchParams.delete("history");
				window.history.replaceState({path:url.href}, '', url.href);
			} else {
				const compressedHistory = LZString.compressToEncodedURIComponent(JSON.stringify(hist));
				const url = new URL(window.location.href);
				url.searchParams.set("history", compressedHistory);
				window.history.replaceState({path:url.href}, '', url.href);
			}
		}'''

	# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¾‹
	examples = {
		"è¦ç´„ (è«–æ–‡)": '''åˆ¶ç´„æ¡ä»¶ã«å¾“ã„ã€ä»¥ä¸‹ã®ç ”ç©¶è«–æ–‡ã§ææ¡ˆã•ã‚Œã¦ã„ã‚‹æŠ€è¡“ã‚„æ‰‹æ³•ã«ã¤ã„ã¦è¦ç´„ã—ã¦ãã ã•ã„ã€‚

# åˆ¶ç´„æ¡ä»¶
* è¦ç´„è€…: å¤§å­¦æ•™æˆ
* æƒ³å®šèª­è€…: å¤§å­¦é™¢ç”Ÿ
* è¦ç´„çµæœã®è¨€èª: æ—¥æœ¬èª
* è¦ç´„çµæœã®æ§‹æˆï¼ˆä»¥ä¸‹ã®å„é …ç›®ã«ã¤ã„ã¦500æ–‡å­—ï¼‰:
  1. ã©ã‚“ãªç ”ç©¶ã§ã‚ã‚‹ã‹
  2. å…ˆè¡Œç ”ç©¶ã«æ¯”ã¹ã¦å„ªã‚Œã¦ã„ã‚‹ç‚¹ã¯ä½•ã‹
  3. ææ¡ˆã•ã‚Œã¦ã„ã‚‹æŠ€è¡“ã‚„æ‰‹æ³•ã®é‡è¦ãªç‚¹ã¯ä½•ã‹
  4. ã©ã®ã‚ˆã†ãªæ–¹æ³•ã§æœ‰åŠ¹ã§ã‚ã‚‹ã¨è©•ä¾¡ã—ãŸã‹
  5. ä½•ã‹è­°è«–ã¯ã‚ã‚‹ã‹
  6. æ¬¡ã«èª­ã‚€ã¹ãè«–æ–‡ã¯ä½•ã‹

# ç ”ç©¶è«–æ–‡
"""
{context}
"""

# è¦ç´„çµæœ''', 
		"è¦ç´„ (ä¸€èˆ¬)": '''åˆ¶ç´„æ¡ä»¶ã«å¾“ã„ã€ä»¥ä¸‹ã®æ–‡æ›¸ã®å†…å®¹ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚

# åˆ¶ç´„æ¡ä»¶
* è¦ç´„è€…: æŠ€è¡“ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ
* æƒ³å®šèª­è€…: çµŒå–¶å±¤ã€CTOã€CIO
* å½¢å¼: ç®‡æ¡æ›¸ã
* åˆ†é‡: 20é …ç›®
* è¦ç´„çµæœã®è¨€èª: æ—¥æœ¬èª

# æ–‡æ›¸
"""
{context}
"""

# è¦ç´„''', 
		"æƒ…å ±æŠ½å‡º": '''åˆ¶ç´„æ¡ä»¶ã«å¾“ã„ã€ä»¥ä¸‹ã®æ–‡æ›¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

# åˆ¶ç´„æ¡ä»¶
* æŠ½å‡ºã™ã‚‹æƒ…å ±: èª²é¡Œã‚„å•é¡Œç‚¹ã«ã¤ã„ã¦è¨€åŠã—ã¦ã„ã‚‹å…¨ã¦ã®æ–‡ã€‚ä¸€ã¤ã‚‚è¦‹è½ã¨ã•ãªã„ã§ãã ã•ã„ã€‚
* å‡ºåŠ›å½¢å¼: ç®‡æ¡æ›¸ã
* å‡ºåŠ›è¨€èª: å…ƒã®è¨€èªã®æ–‡ç« ã¨ã€ãã®æ—¥æœ¬èªè¨³

# æ–‡æ›¸
"""
{context}
"""

# æŠ½å‡ºçµæœ''', 
		"QA (æ—¥æœ¬èªæ–‡æ›¸RAG)": '''æ¬¡ã®è³ªå•ã«å›ç­”ã™ã‚‹ãŸã‚ã«å½¹ç«‹ã¤ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢ã—ã¦ã€ãã®æ¤œç´¢çµæœã‚’ä½¿ã£ã¦å›ç­”ã—ã¦ä¸‹ã•ã„ã€‚

# åˆ¶ç´„æ¡ä»¶
* æ¤œç´¢ã‚¯ã‚¨ãƒªã®ç”Ÿæˆæ–¹æ³•: è³ªå•æ–‡ã®3ã¤ã®è¨€ã„æ›ãˆï¼ˆparaphraseï¼‰ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§é€£çµã—ãŸæ–‡å­—åˆ—
* æ¤œç´¢ã‚¯ã‚¨ãƒªã®è¨€èª: æ—¥æœ¬èª
* æ¤œç´¢ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°: 3
* å›ç­”æ–¹æ³•: 
  - æ¤œç´¢çµæœã®æƒ…å ±ã®ã¿ã‚’ç”¨ã„ã¦å›ç­”ã™ã‚‹ã“ã¨ã€‚
  - å›ç­”ã«åˆ©ç”¨ã—ãŸæ–‡ç« ã®ã‚ã‚‹ãƒšãƒ¼ã‚¸ç•ªå·ã‚’ã€ãã‚Œãã‚Œã®å›ç­”æ–‡ã®æ–‡æœ«ã«ä»˜ä¸ã™ã‚‹ã“ã¨ã€‚å½¢å¼: "ï¼ˆå‚è€ƒãƒšãƒ¼ã‚¸ç•ªå·: 71, 59, 47ï¼‰"
  - å›ç­”ã«å½¹ç«‹ã¤æƒ…å ±ãŒæ¤œç´¢çµæœå†…ã«ãªã„å ´åˆã¯ã€Œæ¤œç´¢çµæœã«ã¯å›ç­”ã«å½¹ç«‹ã¤æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã€ã¨å›ç­”ã™ã‚‹ã“ã¨ã€‚
* å›ç­”ã®è¨€èª: æ—¥æœ¬èª

# è³ªå•
ã©ã®ã‚ˆã†ãªæ–¹æ³•ã§ã€ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•ãŒæœ‰åŠ¹ã§ã‚ã‚‹ã¨è©•ä¾¡ã—ã¾ã—ãŸã‹ï¼Ÿ

# å›ç­”''', 
		"QA (è‹±èªæ–‡æ›¸RAG)": '''æ¬¡ã®è³ªå•ã«å›ç­”ã™ã‚‹ãŸã‚ã«å½¹ç«‹ã¤ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢ã—ã¦ã€ãã®æ¤œç´¢çµæœã‚’ä½¿ã£ã¦å›ç­”ã—ã¦ä¸‹ã•ã„ã€‚

# åˆ¶ç´„æ¡ä»¶
* æ¤œç´¢ã‚¯ã‚¨ãƒªã®ç”Ÿæˆæ–¹æ³•: è³ªå•æ–‡ã®3ã¤ã®è¨€ã„æ›ãˆï¼ˆparaphraseï¼‰ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§é€£çµã—ãŸæ–‡å­—åˆ—
* æ¤œç´¢ã‚¯ã‚¨ãƒªã®è¨€èª: è‹±èª
* æ¤œç´¢ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°: 3
* å›ç­”æ–¹æ³•: 
  - æ¤œç´¢çµæœã®æƒ…å ±ã®ã¿ã‚’ç”¨ã„ã¦å›ç­”ã™ã‚‹ã“ã¨ã€‚
  - å›ç­”ã«åˆ©ç”¨ã—ãŸæ–‡ç« ã®ã‚ã‚‹ãƒšãƒ¼ã‚¸ç•ªå·ã‚’ã€ãã‚Œãã‚Œã®å›ç­”æ–‡ã®æ–‡æœ«ã«ä»˜ä¸ã™ã‚‹ã“ã¨ã€‚å½¢å¼: "ï¼ˆå‚è€ƒãƒšãƒ¼ã‚¸ç•ªå·: 71, 59, 47ï¼‰"
  - å›ç­”ã«å½¹ç«‹ã¤æƒ…å ±ãŒæ¤œç´¢çµæœå†…ã«ãªã„å ´åˆã¯ã€Œæ¤œç´¢çµæœã«ã¯å›ç­”ã«å½¹ç«‹ã¤æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã€ã¨å›ç­”ã™ã‚‹ã“ã¨ã€‚
* å›ç­”ã®è¨€èª: æ—¥æœ¬èª

# è³ªå•
ã©ã®ã‚ˆã†ãªæ–¹æ³•ã§ã€ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•ãŒæœ‰åŠ¹ã§ã‚ã‚‹ã¨è©•ä¾¡ã—ã¾ã—ãŸã‹ï¼Ÿ

# å›ç­”''', 
		"è¦ç´„ (RAG)": '''æ¬¡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢ã—ã¦ã€ãã®æ¤œç´¢çµæœã‚’ãƒšãƒ¼ã‚¸ã”ã¨ã«è¦ç´„ã—ã¦ä¸‹ã•ã„ã€‚

# åˆ¶ç´„æ¡ä»¶
* ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: dataset datasets
* æ¤œç´¢ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°: 3
* è¦ç´„çµæœã®è¨€èª: æ—¥æœ¬èª
* è¦ç´„ã®å½¢å¼:
  ## ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆä¾‹: 12ãƒšãƒ¼ã‚¸ï¼‰
  - è¦ç´„æ–‡1
  - è¦ç´„æ–‡2
  ...
* è¦ç´„ã®åˆ†é‡: å„ãƒšãƒ¼ã‚¸3é …ç›®

# è¦ç´„''', 
		"ç¿»è¨³ (RAG)": '''æ¬¡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢ã—ã¦ã€ãã®æ¤œç´¢çµæœã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ä¸‹ã•ã„ã€‚

# åˆ¶ç´„æ¡ä»¶
* ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: dataset datasets
* æ¤œç´¢ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°: 1

# ç¿»è¨³çµæœ''', 
		"è¦ç´„ (ãƒšãƒ¼ã‚¸æŒ‡å®š)": '''16ã€œ17ãƒšãƒ¼ã‚¸ã‚’ãƒšãƒ¼ã‚¸ã”ã¨ã«ç®‡æ¡æ›¸ãã§è¦ç´„ã—ã¦ä¸‹ã•ã„ã€‚

# åˆ¶ç´„æ¡ä»¶
* è¦ç´„çµæœã®è¨€èª: æ—¥æœ¬èª
* è¦ç´„ã®å½¢å¼:
  ## ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆä¾‹: 12ãƒšãƒ¼ã‚¸ï¼‰
  - è¦ç´„æ–‡1
  - è¦ç´„æ–‡2
  ...
* è¦ç´„ã®åˆ†é‡: å„ãƒšãƒ¼ã‚¸5é …ç›®

# è¦ç´„''', 
		"ç¶šãã‚’ç”Ÿæˆ": "ç¶šãã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
	}

	with gr.Blocks(theme=gr.themes.Default(), analytics_enabled=False) as app:
		with gr.Tabs():
			with gr.TabItem("Settings"):
				with gr.Column():
					with gr.Column(variant="panel"):
						with gr.Row():
							setting_name = gr.Textbox(label="Setting Name", value="Default", interactive=True)
							setting_name.change(None, inputs=setting_name, outputs=None, 
								js='(x) =&gt; saveItem("setting_name", x)', show_progress="hidden")

						with gr.Row():
							platform = gr.Radio(label="Platform", interactive=True, 
								choices=["OpenAI", "Azure"], value="OpenAI")
							platform.change(None, inputs=platform, outputs=None, 
								js='(x) =&gt; saveItem("platform", x)', show_progress="hidden")

						with gr.Row():
							endpoint = gr.Textbox(label="Endpoint", interactive=True)
							endpoint.change(None, inputs=endpoint, outputs=None, 
								js='(x) =&gt; saveItem("endpoint", x)', show_progress="hidden")

							azure_deployment = gr.Textbox(label="Azure Deployment", interactive=True)
							azure_deployment.change(None, inputs=azure_deployment, outputs=None, 
								js='(x) =&gt; saveItem("azure_deployment", x)', show_progress="hidden")

							azure_api_version = gr.Textbox(label="Azure API Version", interactive=True)
							azure_api_version.change(None, inputs=azure_api_version, outputs=None, 
								js='(x) =&gt; saveItem("azure_api_version", x)', show_progress="hidden")

						with gr.Group():
							with gr.Row():
								api_key_file = gr.File(file_count="single", file_types=["text"], 
									height=80, label="API Key File")
								api_key = gr.Textbox(label="API Key", type="password", interactive=True)
								# æ³¨æ„: ç§˜å¯†æƒ…å ±ã‚’localStorageã«ä¿å­˜ã—ã¦ã¯ãªã‚‰ãªã„ã€‚ä»–è€…ã«ç§˜å¯†æƒ…å ±ãŒç›—ã¾ã‚Œã‚‹å±é™ºæ€§ãŒã‚ã‚‹ã‹ã‚‰ã§ã‚ã‚‹ã€‚

								api_key_file.upload(load_api_key, inputs=api_key_file, outputs=api_key, 
									show_progress="hidden")
								api_key_file.clear(lambda: None, inputs=None, outputs=api_key, show_progress="hidden")

						with gr.Row():
							model_name = gr.Textbox(label="Model", interactive=True)
							model_name.change(None, inputs=model_name, outputs=None, 
								js='(x) =&gt; saveItem("model_name", x)', show_progress="hidden")

							max_tokens = gr.Number(label="Max Tokens", interactive=True,
								minimum=0, precision=0, step=1)
							max_tokens.change(None, inputs=max_tokens, outputs=None, 
								js='(x) =&gt; saveItem("max_tokens", x)', show_progress="hidden")

							temperature = gr.Slider(label="Temperature", interactive=True,
								minimum=0.0, maximum=1.0, step=0.1)
							temperature.change(None, inputs=temperature, outputs=None, 
								js='(x) =&gt; saveItem("temperature", x)', show_progress="hidden")

						save_chat_history_to_url = gr.Checkbox(label="Save Chat History to URL", interactive=True)

						reset_button = gr.Button("Reset Settings")

					with gr.Column(variant="panel"):
						default_saved_settings = list(DEFAULT_SETTINGS.values())

						saved_settings_df = gr.Dataframe(
								elem_id="saved_settings",
								value=[default_saved_settings], 
								headers=["Name", "Platform", "Endpoint", "Azure Deployment", "Azure API Version", "Model", "Max Tokens", "Temperature", "Save Chat History to URL"],
								row_count=(0, "dynamic"),
								col_count=(9, "fixed"),
								datatype=["str", "str", "str", "str", "str", "str", "number", "number", "bool"],
								type="array",
								label="Saved Settings",
								show_label=True,
								interactive=False
							)
						selected_setting = gr.State(None)
						temp_selected_row_index = gr.JSON(value=None, visible=False)


						def select_setting(event: gr.SelectData):
							return (event.index[0], event.index[1]), event.index[0]


						saved_settings_df.select(
								select_setting, inputs=None, outputs=[selected_setting, temp_selected_row_index], queue=False, show_progress="hidden"
							).then(
								None, inputs=temp_selected_row_index, outputs=None, js='(row_index) =&gt; { for (let e of document.querySelectorAll("#saved_settings > div > div > button > svelte-virtual-table-viewport > table > tbody > tr")[row_index].children) { e.classList.add("focus"); } }', queue=False, show_progress="hidden"
							)

						with gr.Row():
							load_saved_settings_button = gr.Button("Load")
							append_or_overwrite_saved_settings_button = gr.Button("Append or Overwrite")
							delete_saved_settings_button = gr.Button("Delete")

						serialized_saved_settings_state = gr.JSON(visible=False)


						def load_saved_setting(saved_settings, selected_setting):
							if not selected_setting:
								return saved_settings

							def u(x):
								return gr.update(value=x, interactive=True)

							row_index = selected_setting[0]

							setting_name, platform, endpoint, azure_deployment, azure_api_version, model_name, max_tokens, temperature, save_chat_history_to_url = saved_settings[row_index]

							return u(setting_name), u(platform), u(endpoint), u(azure_deployment), u(azure_api_version), u(model_name), u(max_tokens), u(temperature), u(save_chat_history_to_url), None


						load_saved_settings_button.click(load_saved_setting, inputs=[saved_settings_df, selected_setting], outputs=[setting_name, platform, endpoint, azure_deployment, azure_api_version, model_name, max_tokens, temperature, save_chat_history_to_url, selected_setting], queue=False, show_progress="hidden")


						def append_or_overwrite_setting(saved_settings, setting_name, platform, endpoint, azure_deployment, azure_api_version, model_name, max_tokens, temperature, save_chat_history_to_url):

							setting_name = setting_name.strip()

							found = False
							new_saved_settings = []
							for entry in saved_settings:
								if entry[0] == setting_name:
									new_saved_settings.append([setting_name, platform, endpoint, azure_deployment, azure_api_version,model_name, max_tokens, temperature, save_chat_history_to_url])
									found = True
								else:
									new_saved_settings.append(entry)

							if not found:
								new_saved_settings.append([setting_name, platform, endpoint, azure_deployment, azure_api_version,model_name, max_tokens, temperature, save_chat_history_to_url])

							return new_saved_settings, None


						def serialize_saved_settings(saved_settings):
							serialization_keys = list(DEFAULT_SETTINGS.keys())

							serialized_saved_settings = [
									{ k: entry[i] for i, k in enumerate(serialization_keys) } 
									for entry in saved_settings
								]
							return serialized_saved_settings


						append_or_overwrite_saved_settings_button.click(
								append_or_overwrite_setting, inputs=[saved_settings_df, setting_name, platform, endpoint, azure_deployment, azure_api_version,model_name, max_tokens, temperature, save_chat_history_to_url], outputs=[saved_settings_df, selected_setting], queue=False, show_progress="hidden"
							).then(
								serialize_saved_settings, inputs=saved_settings_df, outputs=serialized_saved_settings_state, queue=False, show_progress="hidden",
							).then(
								None, inputs=serialized_saved_settings_state, outputs=None, js='(x) =&gt; saveItem("saved_settings", x)', queue=False, show_progress="hidden"
							)


						def delete_setting(saved_settings, selected_setting):
							if not selected_setting:
								return saved_settings

							row_index = selected_setting[0]
							new_saved_settings = saved_settings[0:row_index] + saved_settings[row_index + 1:]

							if not new_saved_settings:
								new_saved_settings.append(default_saved_settings)

							return new_saved_settings, None


						delete_saved_settings_button.click(
								delete_setting, inputs=[saved_settings_df, selected_setting], outputs=[saved_settings_df, selected_setting], queue=False, show_progress="hidden"
							).then(
								serialize_saved_settings, inputs=saved_settings_df, outputs=serialized_saved_settings_state, queue=False, show_progress="hidden",
							).then(
								None, inputs=serialized_saved_settings_state, outputs=None, js='(x) =&gt; saveItem("saved_settings", x)', queue=False, show_progress="hidden"
							)
						
						temp_saved_settings = gr.JSON(visible=False)
						temp_saved_settings.change(lambda x: x, inputs=temp_saved_settings, outputs=saved_settings_df, queue=False, show_progress="hidden")

						setting_items = [setting_name, platform, endpoint, azure_deployment, azure_api_version, model_name, max_tokens, 
							temperature, save_chat_history_to_url, temp_saved_settings]
						reset_button.click(None, inputs=None, outputs=setting_items, 
							js="() =&gt; resetSettings()", show_progress="hidden")

			with gr.TabItem("Chat"):
				with gr.Row():
					with gr.Column(scale=1):
						pdf_file = gr.File(file_count="single", file_types=[".pdf"], 
							height=80, label="PDF")
						context = gr.Textbox(elem_id="context", label="Context", lines=20, 
							interactive=True, autoscroll=False, show_copy_button=True)
						char_counter = gr.Textbox(label="Statistics", value=get_context_info("", []), 
							lines=2, max_lines=2, interactive=False, container=True)

						pdf_file.upload(update_context_element, inputs=pdf_file, outputs=[context, char_counter], queue=False)
						pdf_file.clear(lambda: None, inputs=None, outputs=context, queue=False, show_progress="hidden")

					with gr.Column(scale=2):

						additional_inputs = [context, platform, endpoint, azure_deployment, azure_api_version, api_key, 
							model_name, max_tokens, temperature]

						with gr.Blocks() as chat:
							gr.Markdown(f"# Chat with your PDF")

							with gr.Column(variant="panel"):
								chatbot = gr.Chatbot(
									CHAT_HISTORY,
									elem_id="chatbot", height=500, show_copy_button=True,
									sanitize_html=True, render_markdown=True, 
									latex_delimiters=[
											# { "left": "$$", "right": "$$", "display": True }, 
											# { "left": "$", "right": "$", "display": False }, 
											{ "left": "\\(", "right": "\\)", "display": False }, 
											{ "left": "\\[", "right": "\\]", "display": True }, 
										], 
									likeable=False, layout="bubble",
									avatar_images=[None, Path("robot.png")])

								message_state = gr.State()
								chatbot_state = gr.State(chatbot.value) if chatbot.value else gr.State([])

								with gr.Group():
									with gr.Row():
										message_textbox = gr.Textbox(placeholder="Type a message...", 
											container=False, show_label=False, autofocus=True, interactive=True, scale=7)

										submit_button = gr.Button("Submit", variant="primary", scale=1, min_width=150)
										stop_button = gr.Button("Stop", variant="stop", visible=False, scale=1, min_width=150)

								cost_info = gr.Textbox(elem_id="cost_info", value=get_cost_info(0),
									lines=1, max_lines=1, interactive=False, container=False, elem_classes="cost_info")

								with gr.Row():
									retry_button = gr.Button("ğŸ”„ Retry", variant="secondary", size="sm")
									undo_button = gr.Button("â†©ï¸ Undo", variant="secondary", size="sm")
									clear_button = gr.Button("ğŸ—‘ï¸ Clear", variant="secondary", size="sm")


							def estimate_message_cost(prompt, history, context):
								token_count = 0
								messages = get_openai_messages(prompt, history, context)
								for message in messages:
									tokens = OPENAI_TOKENIZER.encode(message["content"])
									token_count += len(tokens)

								return gr.update(value=get_cost_info(token_count))


							message_textbox.change(estimate_message_cost, inputs=[message_textbox, chatbot, context], outputs=cost_info, queue=False, show_progress="hidden")

							example_title_textbox = gr.Textbox(visible=False, interactive=True)
							gr.Examples([[k] for k, v in examples.items()], 
								inputs=example_title_textbox, outputs=message_textbox, 
								fn=lambda title: examples[title], run_on_click=True)


						def append_message_to_history(message, history):
							message = escape_latex(message)
							history.append([message, None])
							return history, history


						def undo_chat(history):
							if history:
								message, _ = history.pop()
								message = message or ""
							else:
								message = ""
							return history, history, unescape_latex(message)


						async def submit_message(message, history_with_input, *args):
							history = history_with_input[:-1]
							inputs = [message, history]
							inputs.extend(args)

							generator = process_prompt(*inputs)
							message = escape_latex(message)

							has_response = False
							async for response in generator:
								has_response = True
								response = escape_latex(response)
								update = history + [[message, response]]
								yield update, update

							if not has_response:
								update = history + [[message, None]]
								yield update, update


						submit_triggers = [message_textbox.submit, submit_button.click]

						submit_event = gr.events.on(
								submit_triggers, lambda message: ("", message), inputs=[message_textbox], outputs=[message_textbox, message_state], queue=False
							).then(
								append_message_to_history, inputs=[message_state, chatbot_state], outputs=[chatbot, chatbot_state], queue=False
							).then(
								submit_message, inputs=[message_state, chatbot_state] + additional_inputs, outputs=[chatbot, chatbot_state]
							).then(
								estimate_message_cost, inputs=[message_textbox, chatbot, context], outputs=cost_info, show_progress="hidden"
							)

						for submit_trigger in submit_triggers:
							submit_trigger(lambda: (gr.update(visible=False), gr.update(visible=True)), 
								inputs=None, outputs=[submit_button, stop_button], queue=False)
							submit_event.then(lambda: (gr.update(visible=True), gr.update(visible=False)),
								inputs=None, outputs=[submit_button, stop_button], queue=False)

						stop_button.click(None, inputs=None, outputs=None, cancels=submit_event)

						retry_button.click(
								undo_chat, inputs=[chatbot_state], outputs=[chatbot, chatbot_state, message_state], queue=False
							).then(
								append_message_to_history, inputs=[message_state, chatbot_state], outputs=[chatbot, chatbot_state], queue=False
							).then(
								submit_message, inputs=[message_state, chatbot_state] + additional_inputs, outputs=[chatbot, chatbot_state]
							).then(
								estimate_message_cost, inputs=[message_textbox, chatbot, context], outputs=cost_info, show_progress="hidden"
							)

						undo_button.click(
								undo_chat, inputs=[chatbot_state], outputs=[chatbot, chatbot_state, message_state], queue=False
							).then(
								lambda message: message, inputs=message_state, outputs=message_textbox, queue=False
							).then(
								estimate_message_cost, inputs=[message_textbox, chatbot, context], outputs=cost_info, show_progress="hidden"
							)

						clear_button.click(
								lambda: ([], [], None), inputs=None, outputs=[chatbot, chatbot_state, message_state], 
							queue=False
							).then(
								estimate_message_cost, inputs=[message_textbox, chatbot, context], outputs=cost_info, show_progress="hidden"
							)

						chatbot.change(None, inputs=[chatbot, save_chat_history_to_url], outputs=None, 
							# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ä¿å­˜ã™ã‚‹ã€‚
							js=save_or_delete_chat_history, queue=False, show_progress="hidden")

						save_chat_history_to_url.change(None, inputs=[chatbot, save_chat_history_to_url], outputs=None, 
							js=save_or_delete_chat_history, queue=False, show_progress="hidden")

						context.change(
								count_characters, inputs=context, outputs=char_counter, queue=False, show_progress="hidden"
							).then(
								create_search_engine, inputs=context, outputs=None
							).then(
								estimate_message_cost, inputs=[message_textbox, chatbot, context], outputs=cost_info, show_progress="hidden"
							)

		app.load(None, inputs=None, outputs=setting_items, js=js_define_utilities_and_load_settings, show_progress="hidden")

	app.queue().launch()

main()
			</gradio-file>

			<!-- DALL-Eã‚’ç”¨ã„ã¦ä½œã£ãŸãƒœãƒƒãƒˆã‚¢ã‚¤ã‚³ãƒ³ -->
			<gradio-file name="robot.png" url="https://raw.githubusercontent.com/sonoisa/misc/main/resources/icons/chatbot_icon.png" />
		</gradio-lite>

		<script language="javascript" src="https://cdn.jsdelivr.net/npm/lz-string@1.5.0/libs/lz-string.min.js"></script>
		<script language="javascript">
			(function () {
				// ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ãƒãƒ£ãƒƒãƒˆå±¥æ­´ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ãŸã‚‰ãã‚Œã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€chat_history.jsonãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã™ã€‚
				const url = new URL(window.location.href);

				if (url.searchParams.has("history")) {
					const compressedHistory = url.searchParams.get("history");
					hist = LZString.decompressFromEncodedURIComponent(compressedHistory);

					const chat_history_element = document.querySelector('gradio-file[name="chat_history.json"]');
					chat_history_element.textContent = hist;
				}
			})();
		</script>
		<script type="module" crossorigin src="https://cdn.jsdelivr.net/npm/@gradio/lite@4.14.1/dist/lite.js"></script>
	</body>
</html>